{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üì∞ –ü–æ–ª–Ω—ã–π EDA: –ù–æ–≤–æ—Å—Ç–∏ –∏ –¢–∏–∫–µ—Ä—ã\n",
    "\n",
    "**–¶–µ–ª—å:** –ü—Ä–æ–≤–µ—Å—Ç–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –∏—Ö —Å–≤—è–∑–∏ —Å —Ç–∏–∫–µ—Ä–∞–º–∏:\n",
    "- –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π\n",
    "- –ö–∞—á–µ—Å—Ç–≤–æ –º–∞–ø–ø–∏–Ω–≥–∞ —Ç–∏–∫–µ—Ä–æ–≤\n",
    "- –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π ‚Üî —Å–≤–µ—á–µ–π\n",
    "- –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑\n",
    "- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "\n",
    "**–î–∞–Ω–Ω—ã–µ:**\n",
    "- `data/preprocessed_news/train_news_with_tickers.parquet` - –Ω–æ–≤–æ—Å—Ç–∏ —Å –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–º–∏ —Ç–∏–∫–µ—Ä–∞–º–∏\n",
    "- `data/raw/participants/train_candles.csv` - –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–≤–µ—á–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## üì¶ –ò–º–ø–æ—Ä—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å —Ç–∏–∫–µ—Ä–∞–º–∏\n",
    "news_path = Path('data/preprocessed_news/train_news_with_tickers.parquet')\n",
    "stats_path = Path('data/preprocessed_news/train_news_with_tickers_stats.json')\n",
    "candles_path = Path('data/raw/participants/train_candles.csv')\n",
    "\n",
    "news = pd.read_parquet(news_path)\n",
    "candles = pd.read_csv(candles_path)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
    "with open(stats_path, 'r') as f:\n",
    "    stats = json.load(f)\n",
    "\n",
    "# –ü–∞—Ä—Å–∏–º —Å–ø–∏—Å–∫–∏ —Ç–∏–∫–µ—Ä–æ–≤\n",
    "news['matched_tickers'] = news['matched_tickers'].apply(eval)\n",
    "\n",
    "print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ:\")\n",
    "print(f\"   üì∞ –ù–æ–≤–æ—Å—Ç–∏: {len(news):,} —Å—Ç—Ä–æ–∫\")\n",
    "print(f\"   üìä –°–≤–µ—á–∏: {len(candles):,} —Å—Ç—Ä–æ–∫\")\n",
    "print(f\"\\n–ö–æ–ª–æ–Ω–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π: {news.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "---\n",
    "# 1Ô∏è‚É£ –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–∞—Ç—ã\n",
    "news['publish_date'] = pd.to_datetime(news['publish_date'])\n",
    "candles['begin'] = pd.to_datetime(candles['begin'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìã –ë–ê–ó–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ù–û–í–û–°–¢–ï–ô\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\")\n",
    "print(f\"   –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(news):,}\")\n",
    "print(f\"   –ü–µ—Ä–∏–æ–¥: {news['publish_date'].min().date()} ‚Äî {news['publish_date'].max().date()}\")\n",
    "print(f\"   –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {(news['publish_date'].max() - news['publish_date'].min()).days} –¥–Ω–µ–π\")\n",
    "\n",
    "print(f\"\\nüìù –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "print(f\"   –ü—Ä–æ–ø—É—Å–∫–∏ –≤ title: {news['title'].isna().sum()}\")\n",
    "print(f\"   –ü—Ä–æ–ø—É—Å–∫–∏ –≤ publication: {news['publication'].isna().sum()}\")\n",
    "print(f\"   –î—É–±–ª–∏–∫–∞—Ç—ã: {news.duplicated(subset=['publish_date', 'title']).sum()}\")\n",
    "\n",
    "print(f\"\\nüìè –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤:\")\n",
    "news['title_len'] = news['title'].str.len()\n",
    "news['text_len'] = news['publication'].str.len()\n",
    "\n",
    "print(f\"   –ó–∞–≥–æ–ª–æ–≤–∫–∏:\")\n",
    "print(f\"      –°—Ä–µ–¥–Ω—è—è: {news['title_len'].mean():.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "print(f\"      –ú–µ–¥–∏–∞–Ω–∞: {news['title_len'].median():.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "print(f\"      –ú–∏–Ω/–ú–∞–∫—Å: {news['title_len'].min():.0f} / {news['title_len'].max():.0f}\")\n",
    "\n",
    "print(f\"\\n   –¢–µ–∫—Å—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–π:\")\n",
    "print(f\"      –°—Ä–µ–¥–Ω—è—è: {news['text_len'].mean():.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "print(f\"      –ú–µ–¥–∏–∞–Ω–∞: {news['text_len'].median():.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "print(f\"      –ú–∏–Ω/–ú–∞–∫—Å: {news['text_len'].min():.0f} / {news['text_len'].max():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏\n",
    "news['year'] = news['publish_date'].dt.year\n",
    "news['month'] = news['publish_date'].dt.to_period('M')\n",
    "news['date'] = news['publish_date'].dt.date\n",
    "\n",
    "print(\"\\nüìÖ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥–æ–¥–∞–º:\")\n",
    "yearly_counts = news['year'].value_counts().sort_index()\n",
    "for year, count in yearly_counts.items():\n",
    "    print(f\"   {year}: {count:,} –Ω–æ–≤–æ—Å—Ç–µ–π\")\n",
    "\n",
    "print(f\"\\nüìÜ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–Ω—è–º:\")\n",
    "daily_counts = news.groupby('date').size()\n",
    "print(f\"   –°—Ä–µ–¥–Ω–µ–µ –Ω–æ–≤–æ—Å—Ç–µ–π/–¥–µ–Ω—å: {daily_counts.mean():.1f}\")\n",
    "print(f\"   –ú–µ–¥–∏–∞–Ω–∞ –Ω–æ–≤–æ—Å—Ç–µ–π/–¥–µ–Ω—å: {daily_counts.median():.1f}\")\n",
    "print(f\"   –ú–∞–∫—Å –Ω–æ–≤–æ—Å—Ç–µ–π/–¥–µ–Ω—å: {daily_counts.max()}\")\n",
    "print(f\"   –î–Ω–µ–π –±–µ–∑ –Ω–æ–≤–æ—Å—Ç–µ–π: {(daily_counts == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "---\n",
    "# 2Ô∏è‚É£ –ê–Ω–∞–ª–∏–∑ –º–∞–ø–ø–∏–Ω–≥–∞ —Ç–∏–∫–µ—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ticker-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üéØ –ê–ù–ê–õ–ò–ó –ú–ê–ü–ü–ò–ù–ì–ê –¢–ò–ö–ï–†–û–í\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# –ü–æ–∫—Ä—ã—Ç–∏–µ\n",
    "news_with_tickers = news['has_ticker'].sum()\n",
    "coverage_pct = news['has_ticker'].mean() * 100\n",
    "\n",
    "print(f\"\\nüìä –ü–æ–∫—Ä—ã—Ç–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞:\")\n",
    "print(f\"   –ù–æ–≤–æ—Å—Ç–µ–π —Å —Ç–∏–∫–µ—Ä–∞–º–∏: {news_with_tickers:,} ({coverage_pct:.1f}%)\")\n",
    "print(f\"   –ù–æ–≤–æ—Å—Ç–µ–π –±–µ–∑ —Ç–∏–∫–µ—Ä–æ–≤: {len(news) - news_with_tickers:,} ({100-coverage_pct:.1f}%)\")\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–∫–µ—Ä–∞–º –∏–∑ JSON\n",
    "print(f\"\\nüìà –¢–æ–ø-10 —Ç–∏–∫–µ—Ä–æ–≤ –ø–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º:\")\n",
    "ticker_counts = stats['ticker_counts']\n",
    "sorted_tickers = sorted(ticker_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (ticker, count) in enumerate(sorted_tickers[:10], 1):\n",
    "    print(f\"   {i:2}. {ticker}: {count:,} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π\")\n",
    "\n",
    "# –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è\n",
    "news['num_tickers'] = news['matched_tickers'].apply(len)\n",
    "print(f\"\\nüîó –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è:\")\n",
    "print(f\"   –ù–æ–≤–æ—Å—Ç–µ–π —Å 1 —Ç–∏–∫–µ—Ä–æ–º: {(news['num_tickers'] == 1).sum():,}\")\n",
    "print(f\"   –ù–æ–≤–æ—Å—Ç–µ–π —Å 2+ —Ç–∏–∫–µ—Ä–∞–º–∏: {(news['num_tickers'] >= 2).sum():,}\")\n",
    "print(f\"   –°—Ä–µ–¥–Ω–µ–µ —Ç–∏–∫–µ—Ä–æ–≤/–Ω–æ–≤–æ—Å—Ç—å: {news[news['has_ticker']]['num_tickers'].mean():.2f}\")\n",
    "print(f\"   –ú–∞–∫—Å–∏–º—É–º —Ç–∏–∫–µ—Ä–æ–≤ –≤ –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏: {news['num_tickers'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ticker-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π\n",
    "multi_ticker_dist = news['num_tickers'].value_counts().sort_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–∏–∫–µ—Ä–æ–≤ –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å\n",
    "multi_ticker_dist.plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∏–∫–µ—Ä–æ–≤ –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∏–∫–µ—Ä–æ–≤')\n",
    "axes[0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# –¢–æ–ø-15 —Ç–∏–∫–µ—Ä–æ–≤ –ø–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º\n",
    "top_tickers = sorted_tickers[:15]\n",
    "tickers_df = pd.DataFrame(top_tickers, columns=['Ticker', 'Count'])\n",
    "axes[1].barh(tickers_df['Ticker'], tickers_df['Count'], color='coral')\n",
    "axes[1].set_title('–¢–æ–ø-15 —Ç–∏–∫–µ—Ä–æ–≤ –ø–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "---\n",
    "# 3Ô∏è‚É£ –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π ‚Üî —Å–≤–µ—á–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ticker-overlap",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üîÑ –ü–ï–†–ï–°–ï–ß–ï–ù–ò–Ø –ù–û–í–û–°–¢–ï–ô ‚Üî –°–í–ï–ß–ï–ô\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# –¢–∏–∫–µ—Ä—ã –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π\n",
    "all_news_tickers = set(ticker_counts.keys())\n",
    "\n",
    "# –¢–∏–∫–µ—Ä—ã –∏–∑ —Å–≤–µ—á–µ–π\n",
    "all_candles_tickers = set(candles['ticker'].unique())\n",
    "\n",
    "print(f\"\\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–∏–∫–µ—Ä–æ–≤:\")\n",
    "print(f\"   –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–∏–∫–µ—Ä—ã –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö: {len(all_news_tickers)}\")\n",
    "print(f\"   –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–∏–∫–µ—Ä—ã –≤ —Å–≤–µ—á–∞—Ö: {len(all_candles_tickers)}\")\n",
    "\n",
    "only_news = all_news_tickers - all_candles_tickers\n",
    "only_candles = all_candles_tickers - all_news_tickers\n",
    "common = all_news_tickers & all_candles_tickers\n",
    "\n",
    "print(f\"\\n‚úÖ –û–±—â–∏–µ —Ç–∏–∫–µ—Ä—ã: {len(common)} / {len(all_candles_tickers)} = {len(common)/len(all_candles_tickers)*100:.1f}%\")\n",
    "if only_news:\n",
    "    print(f\"‚ö†Ô∏è  –¢–æ–ª—å–∫–æ –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö: {only_news}\")\n",
    "if only_candles:\n",
    "    print(f\"‚ö†Ô∏è  –¢–æ–ª—å–∫–æ –≤ —Å–≤–µ—á–∞—Ö: {only_candles}\")\n",
    "\n",
    "# –í—Ä–µ–º–µ–Ω–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ\n",
    "news_date_range = (news['publish_date'].min(), news['publish_date'].max())\n",
    "candles_date_range = (candles['begin'].min(), candles['begin'].max())\n",
    "\n",
    "print(f\"\\nüìÖ –í—Ä–µ–º–µ–Ω–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ:\")\n",
    "print(f\"   –ù–æ–≤–æ—Å—Ç–∏: {news_date_range[0].date()} ‚Äî {news_date_range[1].date()}\")\n",
    "print(f\"   –°–≤–µ—á–∏:   {candles_date_range[0].date()} ‚Äî {candles_date_range[1].date()}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∞–≥–∞ (–Ω–æ–≤–æ—Å—Ç–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å <= t-1)\n",
    "print(f\"\\n‚è±Ô∏è  –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∞–≥–∞ (–Ω–æ–≤–æ—Å—Ç–∏ ‚â§ t-1):\")\n",
    "news_dates = set(news['publish_date'].dt.date)\n",
    "candles_dates = set(candles['begin'].dt.date)\n",
    "print(f\"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö: {len(news_dates)}\")\n",
    "print(f\"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç –≤ —Å–≤–µ—á–∞—Ö: {len(candles_dates)}\")\n",
    "print(f\"   –ü–µ—Ä–µ—Å–µ–∫–∞—é—â–∏—Ö—Å—è –¥–∞—Ç: {len(news_dates & candles_dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coverage-per-ticker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–∫—Ä—ã—Ç–∏–µ –Ω–æ–≤–æ—Å—Ç—è–º–∏ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–µ—Ä–∞\n",
    "print(\"\\nüì∞ –ü–æ–∫—Ä—ã—Ç–∏–µ –Ω–æ–≤–æ—Å—Ç—è–º–∏ –ø–æ —Ç–∏–∫–µ—Ä–∞–º:\")\n",
    "print(\"\\n–¢–∏–∫–µ—Ä | –ù–æ–≤–æ—Å—Ç–∏ | –°–≤–µ—á–∏ | –ü–æ–∫—Ä—ã—Ç–∏–µ\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "coverage_data = []\n",
    "for ticker in sorted(all_candles_tickers):\n",
    "    news_count = ticker_counts.get(ticker, 0)\n",
    "    candles_count = len(candles[candles['ticker'] == ticker])\n",
    "    \n",
    "    coverage_data.append({\n",
    "        'Ticker': ticker,\n",
    "        'News': news_count,\n",
    "        'Candles': candles_count\n",
    "    })\n",
    "    \n",
    "    print(f\"{ticker:5} | {news_count:6,} | {candles_count:5,} | {'‚úÖ' if news_count > 0 else '‚ùå'}\")\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "---\n",
    "# 4Ô∏è‚É£ –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"‚è∞ –í–†–ï–ú–ï–ù–ù–û–ô –ê–ù–ê–õ–ò–ó\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# –ß–∞—Å—Ç–æ—Ç–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –º–µ—Å—è—Ü–∞–º\n",
    "monthly_counts = news.groupby('month').size()\n",
    "\n",
    "print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–µ—Å—è—Ü–∞–º:\")\n",
    "print(f\"   –°—Ä–µ–¥–Ω–µ–µ –Ω–æ–≤–æ—Å—Ç–µ–π/–º–µ—Å—è—Ü: {monthly_counts.mean():.0f}\")\n",
    "print(f\"   –ú–µ–¥–∏–∞–Ω–∞: {monthly_counts.median():.0f}\")\n",
    "print(f\"   –ú–∏–Ω/–ú–∞–∫—Å: {monthly_counts.min()} / {monthly_counts.max()}\")\n",
    "\n",
    "# –ú–µ—Å—è—Ü —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é\n",
    "max_month = monthly_counts.idxmax()\n",
    "max_count = monthly_counts.max()\n",
    "print(f\"\\nüìà –ü–∏–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {max_month} ({max_count} –Ω–æ–≤–æ—Å—Ç–µ–π)\")\n",
    "\n",
    "# –ú–µ—Å—è—Ü —Å –Ω–∞–∏–º–µ–Ω—å—à–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é\n",
    "min_month = monthly_counts.idxmin()\n",
    "min_count = monthly_counts.min()\n",
    "print(f\"üìâ –ú–∏–Ω–∏–º—É–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {min_month} ({min_count} –Ω–æ–≤–æ—Å—Ç–µ–π)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# –ì—Ä–∞—Ñ–∏–∫ 1: –û–±—â–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–µ–π\n",
    "monthly_counts_df = monthly_counts.to_frame(name='count')\n",
    "monthly_counts_df.index = monthly_counts_df.index.to_timestamp()\n",
    "monthly_counts_df.plot(ax=axes[0], color='steelblue', linewidth=2, legend=False)\n",
    "axes[0].set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –º–µ—Å—è—Ü–∞–º (2020-2025)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].axhline(monthly_counts.mean(), color='red', linestyle='--', alpha=0.5, label='–°—Ä–µ–¥–Ω–µ–µ')\n",
    "axes[0].legend()\n",
    "\n",
    "# –ì—Ä–∞—Ñ–∏–∫ 2: –¢–æ–ø-5 —Ç–∏–∫–µ—Ä–æ–≤ –ø–æ –º–µ—Å—è—Ü–∞–º\n",
    "# Explode –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Ç–∏–∫–µ—Ä–∞–º –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n",
    "news_exploded = news.explode('matched_tickers')\n",
    "news_exploded = news_exploded[news_exploded['matched_tickers'].notna()]\n",
    "news_exploded['month_ts'] = news_exploded['publish_date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "top_5_tickers = [t[0] for t in sorted_tickers[:5]]\n",
    "for ticker in top_5_tickers:\n",
    "    ticker_news = news_exploded[news_exploded['matched_tickers'] == ticker]\n",
    "    ticker_monthly = ticker_news.groupby('month_ts').size()\n",
    "    ticker_monthly.plot(ax=axes[1], label=ticker, linewidth=2, alpha=0.8)\n",
    "\n",
    "axes[1].set_title('–¢–æ–ø-5 —Ç–∏–∫–µ—Ä–æ–≤: –¥–∏–Ω–∞–º–∏–∫–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('–î–∞—Ç–∞')\n",
    "axes[1].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π')\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "---\n",
    "# 5Ô∏è‚É£ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: Co-occurrence –∏ —Å–µ—Ç–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "co-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üîó CO-OCCURRENCE –ê–ù–ê–õ–ò–ó\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# –°—Ç—Ä–æ–∏–º –º–∞—Ç—Ä–∏—Ü—É co-occurrence (–∫–∞–∫–∏–µ —Ç–∏–∫–µ—Ä—ã —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –≤–º–µ—Å—Ç–µ)\n",
    "tickers_list = sorted(all_candles_tickers)\n",
    "cooccurrence_matrix = pd.DataFrame(0, index=tickers_list, columns=tickers_list)\n",
    "\n",
    "# –ó–∞–ø–æ–ª–Ω—è–µ–º –º–∞—Ç—Ä–∏—Ü—É\n",
    "for tickers in news['matched_tickers']:\n",
    "    if len(tickers) >= 2:\n",
    "        for i, t1 in enumerate(tickers):\n",
    "            for t2 in tickers[i+1:]:\n",
    "                if t1 in tickers_list and t2 in tickers_list:\n",
    "                    cooccurrence_matrix.loc[t1, t2] += 1\n",
    "                    cooccurrence_matrix.loc[t2, t1] += 1\n",
    "\n",
    "print(f\"\\nüìä –ú–∞—Ç—Ä–∏—Ü–∞ co-occurrence –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ ({len(tickers_list)} x {len(tickers_list)})\")\n",
    "print(f\"   –í—Å–µ–≥–æ —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π: {cooccurrence_matrix.sum().sum() / 2:.0f}\")\n",
    "\n",
    "# –¢–æ–ø-10 –ø–∞—Ä —Ç–∏–∫–µ—Ä–æ–≤\n",
    "print(f\"\\nüîù –¢–æ–ø-10 –ø–∞—Ä —Ç–∏–∫–µ—Ä–æ–≤ (—Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è):\")\n",
    "pairs = []\n",
    "for i in range(len(tickers_list)):\n",
    "    for j in range(i+1, len(tickers_list)):\n",
    "        count = cooccurrence_matrix.iloc[i, j]\n",
    "        if count > 0:\n",
    "            pairs.append((tickers_list[i], tickers_list[j], count))\n",
    "\n",
    "pairs_sorted = sorted(pairs, key=lambda x: x[2], reverse=True)\n",
    "for i, (t1, t2, count) in enumerate(pairs_sorted[:10], 1):\n",
    "    print(f\"   {i:2}. {t1} ‚Üî {t2}: {count} —Ä–∞–∑\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ co-occurrence\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cooccurrence_matrix, \n",
    "    annot=True, \n",
    "    fmt='g', \n",
    "    cmap='YlOrRd', \n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'label': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π'}\n",
    ")\n",
    "plt.title('–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞: —Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ç–∏–∫–µ—Ä–æ–≤', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('–¢–∏–∫–µ—Ä', fontsize=11)\n",
    "plt.ylabel('–¢–∏–∫–µ—Ä', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "network-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–µ—Ç–µ–≤–æ–π –≥—Ä–∞—Ñ (—Ç—Ä–µ–±—É–µ—Ç networkx)\n",
    "try:\n",
    "    import networkx as nx\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–ª—ã (–≤—Å–µ —Ç–∏–∫–µ—Ä—ã)\n",
    "    for ticker in tickers_list:\n",
    "        G.add_node(ticker)\n",
    "    \n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–±—Ä–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ >= 10 —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)\n",
    "    threshold = 10\n",
    "    for t1, t2, count in pairs_sorted:\n",
    "        if count >= threshold:\n",
    "            G.add_edge(t1, t2, weight=count)\n",
    "    \n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "    \n",
    "    # –†–∞–∑–º–µ—Ä —É–∑–ª–æ–≤ = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π\n",
    "    node_sizes = [ticker_counts.get(ticker, 0) for ticker in G.nodes()]\n",
    "    \n",
    "    # –†–∏—Å—É–µ–º\n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos, \n",
    "        node_size=[s/5 for s in node_sizes],  # –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º\n",
    "        node_color='lightblue',\n",
    "        edgecolors='navy',\n",
    "        linewidths=2\n",
    "    )\n",
    "    \n",
    "    # –†–µ–±—Ä–∞ —Å —Ç–æ–ª—â–∏–Ω–æ–π = –≤–µ—Å\n",
    "    edges = G.edges()\n",
    "    weights = [G[u][v]['weight'] for u, v in edges]\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos, \n",
    "        width=[w/30 for w in weights],\n",
    "        alpha=0.5,\n",
    "        edge_color='gray'\n",
    "    )\n",
    "    \n",
    "    # –ü–æ–¥–ø–∏—Å–∏\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "    \n",
    "    plt.title(f'–°–µ—Ç–µ–≤–æ–π –≥—Ä–∞—Ñ: —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–∏–∫–µ—Ä–∞–º–∏ (>= {threshold} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ –°–µ—Ç—å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞: {G.number_of_nodes()} —É–∑–ª–æ–≤, {G.number_of_edges()} —Ä–µ–±–µ—Ä\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  networkx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ: pip install networkx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "---\n",
    "# 6Ô∏è‚É£ –ü—Ä–∏–º–µ—Ä—ã –Ω–æ–≤–æ—Å—Ç–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üì∞ –ü–†–ò–ú–ï–†–´ –ù–û–í–û–°–¢–ï–ô\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 3 —Å–ª—É—á–∞–π–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ —Ç–∏–∫–µ—Ä–∞–º–∏\n",
    "print(\"\\n‚úÖ –ü—Ä–∏–º–µ—Ä—ã —Å —É—Å–ø–µ—à–Ω—ã–º –º–∞–ø–ø–∏–Ω–≥–æ–º:\\n\")\n",
    "successful = news[news['has_ticker']].sample(3, random_state=42)\n",
    "for idx, row in successful.iterrows():\n",
    "    print(f\"üìÖ {row['publish_date'].date()}\")\n",
    "    print(f\"üéØ –¢–∏–∫–µ—Ä—ã: {', '.join(row['matched_tickers'])}\")\n",
    "    print(f\"üì∞ {row['title']}\")\n",
    "    print(f\"üìù {row['publication'][:200]}...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# 3 –Ω–æ–≤–æ—Å—Ç–∏ –±–µ–∑ —Ç–∏–∫–µ—Ä–æ–≤\n",
    "if (~news['has_ticker']).sum() > 0:\n",
    "    print(\"\\n‚ùå –ü—Ä–∏–º–µ—Ä—ã –±–µ–∑ –º–∞–ø–ø–∏–Ω–≥–∞:\\n\")\n",
    "    failed = news[~news['has_ticker']].sample(min(3, (~news['has_ticker']).sum()), random_state=42)\n",
    "    for idx, row in failed.iterrows():\n",
    "        print(f\"üìÖ {row['publish_date'].date()}\")\n",
    "        print(f\"üì∞ {row['title']}\")\n",
    "        print(f\"üìù {row['publication'][:200]}...\")\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "# üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üìã –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n‚úÖ –ö–ê–ß–ï–°–¢–í–û –î–ê–ù–ù–´–•:\")\n",
    "print(f\"   –ù–æ–≤–æ—Å—Ç–µ–π: {len(news):,}\")\n",
    "print(f\"   –ü–æ–∫—Ä—ã—Ç–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞: {coverage_pct:.1f}%\")\n",
    "print(f\"   –¢–∏–∫–µ—Ä–æ–≤: {len(all_candles_tickers)}\")\n",
    "print(f\"   –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–∫–µ—Ä–æ–≤ –Ω–æ–≤–æ—Å—Ç–∏‚Üî—Å–≤–µ—á–∏: {len(common)}/{len(all_candles_tickers)} (100%)\")\n",
    "\n",
    "print(f\"\\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï:\")\n",
    "print(f\"   –°—Ä–µ–¥–Ω–µ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π/—Ç–∏–∫–µ—Ä: {sum(ticker_counts.values()) / len(ticker_counts):.0f}\")\n",
    "print(f\"   –¢–æ–ø —Ç–∏–∫–µ—Ä: {sorted_tickers[0][0]} ({sorted_tickers[0][1]:,} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)\")\n",
    "print(f\"   –°—Ä–µ–¥–Ω–µ–µ –Ω–æ–≤–æ—Å—Ç–µ–π/–¥–µ–Ω—å: {daily_counts.mean():.1f}\")\n",
    "\n",
    "print(f\"\\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\")\n",
    "if coverage_pct < 90:\n",
    "    print(f\"   ‚ö†Ô∏è  –ü–æ–∫—Ä—ã—Ç–∏–µ < 90% - —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —É–ª—É—á—à–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –º–∞–ø–ø–∏–Ω–≥–∞\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ –ü–æ–∫—Ä—ã—Ç–∏–µ >= 90% - –º–∞–ø–ø–∏–Ω–≥ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ\")\n",
    "\n",
    "if len(only_candles) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  –ï—Å—Ç—å —Ç–∏–∫–µ—Ä—ã –±–µ–∑ –Ω–æ–≤–æ—Å—Ç–µ–π: {only_candles}\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ –í—Å–µ —Ç–∏–∫–µ—Ä—ã –∏–∑ —Å–≤–µ—á–µ–π –ø–æ–∫—Ä—ã—Ç—ã –Ω–æ–≤–æ—Å—Ç—è–º–∏\")\n",
    "\n",
    "print(f\"\\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
