# –¢–µ—Å—Ç LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –¥–∞—Ç–∞–º
import json
import time
import re
import pandas as pd
from openai import OpenAI
from datetime import datetime

# ============================================
# –ù–ê–°–¢–†–û–ô–ö–ò - –ò–ó–ú–ï–ù–ò –ó–î–ï–°–¨
# ============================================

API_KEY = "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"

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π OpenRouter
MODELS = {
    "gpt-4o-mini": {
        "id": "openai/gpt-4o-mini",
        "batch_size": 20,
        "input_price": 0.15,
        "output_price": 0.60,
        "max_tokens": 4096
    },
    "deepseek-r1-distill-llama-70b": {
        "id": "deepseek/deepseek-r1-distill-llama-70b",
        "batch_size": 20,
        "input_price": 0.03,
        "output_price": 0.13,
        "max_tokens": 4096
    },
    "qwen-2.5-72b": {
        "id": "qwen/qwen-2.5-72b-instruct",
        "batch_size": 20,
        "input_price": 0.35,
        "output_price": 0.40,
        "max_tokens": 4096
    },
    "mistral-small": {
        "id": "mistralai/mistral-small",
        "batch_size": 20,
        "input_price": 0.04,
        "output_price": 0.40,
        "max_tokens": 4096
    }
}

# ====================================
# –í–´–ë–ï–†–ò –ú–û–î–ï–õ–¨ –ó–î–ï–°–¨
# ====================================
CURRENT_MODEL = "gpt-4o-mini"

# ====================================
# –§–ò–õ–¨–¢–† –ü–û –î–ê–¢–ê–ú
# ====================================
# –£—Å—Ç–∞–Ω–æ–≤–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
# –§–æ—Ä–º–∞—Ç: "YYYY-MM-DD" –∏–ª–∏ None –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
START_DATE = "2025-01-01"  # –ù–∞—á–∞–ª–æ –ø–µ—Ä–∏–æ–¥–∞ (–≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)
END_DATE = "2025-09-08"    # –ö–æ–Ω–µ—Ü –ø–µ—Ä–∏–æ–¥–∞ (–≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)

# –ï—Å–ª–∏ —Ö–æ—á–µ—à—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –í–°–ï –Ω–æ–≤–æ—Å—Ç–∏, —É—Å—Ç–∞–Ω–æ–≤–∏ –æ–±–µ –¥–∞—Ç—ã –≤ None:
# START_DATE = None
# END_DATE = None

# ====================================
# –ü–£–¢–¨ –ö –§–ê–ô–õ–£ –° –ù–û–í–û–°–¢–Ø–ú–ò
# ====================================
INPUT_FILE = "data/preprocessed_news/news_2_with_tickers.csv"

# ============================================

MODEL_CONFIG = MODELS[CURRENT_MODEL]
MODEL = MODEL_CONFIG["id"]
BATCH_SIZE = MODEL_CONFIG["batch_size"]
MAX_TOKENS = MODEL_CONFIG["max_tokens"]

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=API_KEY
)

# –ü—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
SYSTEM_PROMPT = """–¢—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–æ–≤–æ—Å—Ç–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª—è–π –∏—Ö –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –∞–∫—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏.

–î–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –æ–ø—Ä–µ–¥–µ–ª–∏:
1. **sentiment** (—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å):
   - **-1**: –ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å (–ø–∞–¥–µ–Ω–∏–µ –≤—ã—Ä—É—á–∫–∏, —Å–∞–Ω–∫—Ü–∏–∏, —É–±—ã—Ç–∫–∏, —Å—É–¥–µ–±–Ω—ã–µ –∏—Å–∫–∏, –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã)
   - **0**: –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å (–æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –±–µ–∑ —è–≤–Ω–æ–≥–æ –≤–ª–∏—è–Ω–∏—è –Ω–∞ –∞–∫—Ü–∏–∏)
   - **1**: –ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å (—Ä–æ—Å—Ç –ø—Ä–∏–±—ã–ª–∏, –Ω–æ–≤—ã–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã, –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã, –¥–∏–≤–∏–¥–µ–Ω–¥—ã)

2. **confidence** (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –æ—Ç 0 –¥–æ 10):
   - **0-3**: –°–ª–∞–±–æ–µ –≤–ª–∏—è–Ω–∏–µ (—É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –≤ –æ–±—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏)
   - **4-6**: –£–º–µ—Ä–µ–Ω–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ (–∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–µ –æ—Ç—á–µ—Ç—ã, –æ–±—ã—á–Ω—ã–µ —Å–¥–µ–ª–∫–∏)
   - **7-10**: –°–∏–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ (–∫—Ä—É–ø–Ω—ã–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã, —Å–∞–Ω–∫—Ü–∏–∏, —Å–º–µ–Ω–∞ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞, —Ä–µ–∑–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π)

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–¢–†–û–ì–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –º–∞—Å—Å–∏–≤–∞ –ë–ï–ó –ü–û–Ø–°–ù–ï–ù–ò–ô:
[
  {"index": 0, "sentiment": -1, "confidence": 8},
  {"index": 1, "sentiment": 0, "confidence": 3},
  {"index": 2, "sentiment": 1, "confidence": 6}
]"""

def analyze_batch(batch_df):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞—Ç—á –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ API"""

    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –õ–û–ö–ê–õ–¨–ù–´–ú–ò –∏–Ω–¥–µ–∫—Å–∞–º–∏
    prompt = "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:\n\n"
    for local_idx, (orig_idx, row) in enumerate(batch_df.iterrows()):
        prompt += f"**Index {local_idx}** | Ticker: {row['tickers']}\n"
        prompt += f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {row['title']}\n"
        prompt += f"–¢–µ–∫—Å—Ç: {row['publication'][:500]}...\n\n"

    prompt += f"\n–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON –º–∞—Å—Å–∏–≤ –ë–ï–ó –ü–û–Ø–°–ù–ï–ù–ò–ô —Å sentiment –∏ confidence –¥–ª—è –∫–∞–∂–¥–æ–≥–æ index (0-{len(batch_df)-1})."

    try:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            max_tokens=MAX_TOKENS,
            temperature=0.3
        )

        result = response.choices[0].message.content

        # –ü–∞—Ä—Å–∏–º JSON
        try:
            # –£–¥–∞–ª—è–µ–º markdown –±–ª–æ–∫–∏
            if "```json" in result:
                result = result.split("```json")[1].split("```")[0].strip()
            elif "```" in result:
                result = result.split("```")[1].split("```")[0].strip()

            # –ò—â–µ–º JSON –º–∞—Å—Å–∏–≤ —á–µ—Ä–µ–∑ regex
            json_match = re.search(r'\[[\s\S]*\]', result)
            if json_match:
                json_str = json_match.group(0)
                sentiments = json.loads(json_str)
            else:
                sentiments = json.loads(result)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if len(sentiments) != len(batch_df):
                print(f"  ‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω–æ {len(sentiments)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–º–µ—Å—Ç–æ {len(batch_df)}")
                # –î–æ–ø–æ–ª–Ω—è–µ–º –ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                while len(sentiments) < len(batch_df):
                    sentiments.append({"sentiment": None, "confidence": None})

            usage = response.usage
            return sentiments, usage

        except json.JSONDecodeError as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            print(f"  –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {result[:200]}...")
            return [{"sentiment": None, "confidence": None}] * len(batch_df), None

    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ API: {e}")
        return [{"sentiment": None, "confidence": None}] * len(batch_df), None

def calculate_cost(input_tokens, output_tokens):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç—å –¥–ª—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
    input_cost = (input_tokens / 1_000_000) * MODEL_CONFIG["input_price"]
    output_cost = (output_tokens / 1_000_000) * MODEL_CONFIG["output_price"]
    return input_cost, output_cost, input_cost + output_cost

# ============================================
# –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
# ============================================

try:
    news_df = pd.read_csv(INPUT_FILE)
except FileNotFoundError:
    print(f"‚ùå –§–∞–π–ª {INPUT_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    print("–£–∫–∞–∂–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π INPUT_FILE")
    exit(1)

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –¥–∞—Ç–∞–º–∏ –≤ datetime
news_df['publish_date'] = pd.to_datetime(news_df['publish_date'])

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π
original_count = len(news_df)

# –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–∞—Ç–∞–º
if START_DATE is not None or END_DATE is not None:
    if START_DATE is not None:
        start_dt = pd.to_datetime(START_DATE)
        news_df = news_df[news_df['publish_date'] >= start_dt]

    if END_DATE is not None:
        end_dt = pd.to_datetime(END_DATE) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)  # –î–æ –∫–æ–Ω—Ü–∞ –¥–Ω—è
        news_df = news_df[news_df['publish_date'] <= end_dt]

    if START_DATE and END_DATE:
        date_range_str = f"{START_DATE} - {END_DATE}"
    elif START_DATE:
        date_range_str = f"—Å {START_DATE}"
    else:
        date_range_str = f"–¥–æ {END_DATE}"
else:
    date_range_str = "–≤—Å–µ –¥–∞—Ç—ã"

# –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
news_df = news_df.reset_index(drop=True)

if len(news_df) == 0:
    print(f"‚ùå –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –¥–∞—Ç–∞–º –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –Ω–æ–≤–æ—Å—Ç–µ–π!")
    exit(1)

print(f"\n{'='*60}")
print(f"üöÄ –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê")
print(f"{'='*60}")
print(f"–ú–æ–¥–µ–ª—å: {CURRENT_MODEL} ({MODEL})")
print(f"Batch size: {BATCH_SIZE}")
print(f"üìÖ –ü–µ—Ä–∏–æ–¥: {date_range_str}")
print(f"üìä –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {len(news_df)} –∏–∑ {original_count} –Ω–æ–≤–æ—Å—Ç–µ–π")
print(f"–ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {(len(news_df)-1)//BATCH_SIZE + 1}")
print(f"\nüí∞ –¶–ï–ù–´ –ú–û–î–ï–õ–ò:")
print(f"  Input:  ${MODEL_CONFIG['input_price']:.3f} / 1M —Ç–æ–∫–µ–Ω–æ–≤")
print(f"  Output: ${MODEL_CONFIG['output_price']:.3f} / 1M —Ç–æ–∫–µ–Ω–æ–≤")
print(f"{'='*60}\n")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–ª–æ–Ω–∫–∏
news_df['sentiment'] = None
news_df['confidence'] = None

# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
total_input_tokens = 0
total_output_tokens = 0
start_time = time.time()

for i in range(0, len(news_df), BATCH_SIZE):
    batch = news_df.iloc[i:i+BATCH_SIZE]
    batch_num = i//BATCH_SIZE + 1
    total_batches = (len(news_df)-1)//BATCH_SIZE + 1

    print(f"–ë–∞—Ç—á {batch_num}/{total_batches} ({i}-{min(i+BATCH_SIZE, len(news_df))})...", end=" ")

    sentiments, usage = analyze_batch(batch)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for local_idx, sent_data in enumerate(sentiments):
        global_idx = i + local_idx
        if global_idx < len(news_df):
            news_df.loc[global_idx, 'sentiment'] = sent_data.get('sentiment')
            news_df.loc[global_idx, 'confidence'] = sent_data.get('confidence')

    if usage:
        total_input_tokens += usage.prompt_tokens
        total_output_tokens += usage.completion_tokens

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–æ–∏–º–æ—Å—Ç—å –Ω–∞ —Ç–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç
        input_cost, output_cost, total_cost = calculate_cost(total_input_tokens, total_output_tokens)

        print(f"‚úì {usage.prompt_tokens}‚Üë/{usage.completion_tokens}‚Üì | –ü–æ—Ç—Ä–∞—á–µ–Ω–æ: ${total_cost:.4f}")
    else:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    start_str = START_DATE.replace('-', '') if START_DATE else 'all'
    temp_filename = f'news_2_with_tickers_llm_{start_str}_TEMP.csv'
    news_df.to_csv(temp_filename, index=False)

    time.sleep(0.5)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

elapsed_time = time.time() - start_time

# ============================================
# –†–ï–ó–£–õ–¨–¢–ê–¢–´
# ============================================

print(f"\n{'='*60}")
print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ê")
print(f"{'='*60}")
print(f"–ú–æ–¥–µ–ª—å: {CURRENT_MODEL}")
print(f"–ü–µ—Ä–∏–æ–¥: {date_range_str}")
print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(news_df)}")
print(f"–£—Å–ø–µ—à–Ω—ã—Ö: {news_df['sentiment'].notna().sum()}")
print(f"–û—à–∏–±–æ–∫: {news_df['sentiment'].isna().sum()}")
print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed_time/60:.2f} –º–∏–Ω—É—Ç")

print(f"\nüìà –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–û –¢–û–ö–ï–ù–û–í:")
print(f"  Input:  {total_input_tokens:,}")
print(f"  Output: {total_output_tokens:,}")
print(f"  TOTAL:  {total_input_tokens + total_output_tokens:,}")

# –ü–æ–¥—Å—á–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏
if total_input_tokens > 0:
    input_cost, output_cost, total_cost = calculate_cost(total_input_tokens, total_output_tokens)

    print(f"\nüí∞ –°–¢–û–ò–ú–û–°–¢–¨ ({CURRENT_MODEL}):")
    print(f"  Input:  ${input_cost:.6f}")
    print(f"  Output: ${output_cost:.6f}")
    print(f"  TOTAL:  ${total_cost:.6f}")

    # –ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (60k –Ω–æ–≤–æ—Å—Ç–µ–π)
    if original_count > len(news_df):
        scale_factor = original_count / len(news_df)
        projected_input = total_input_tokens * scale_factor
        projected_output = total_output_tokens * scale_factor
        proj_in_cost, proj_out_cost, proj_total = calculate_cost(projected_input, projected_output)
        projected_time = elapsed_time * scale_factor

        print(f"\nüîÆ –ü–†–û–ì–ù–û–ó –î–õ–Ø –í–°–ï–• {original_count:,} –ù–û–í–û–°–¢–ï–ô:")
        print(f"  –¢–æ–∫–µ–Ω—ã: {projected_input:,.0f} input / {projected_output:,.0f} output")
        print(f"  –°—Ç–æ–∏–º–æ—Å—Ç—å: ${proj_total:.2f}")
        print(f"  –í—Ä–µ–º—è: ~{projected_time/60:.1f} –º–∏–Ω—É—Ç")

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
valid_sentiments = news_df[news_df['sentiment'].notna()]
if len(valid_sentiments) > 0:
    print(f"\nüìã –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï SENTIMENT:")
    sentiment_counts = valid_sentiments['sentiment'].value_counts().sort_index()
    for sent, count in sentiment_counts.items():
        label = {-1: "üìâ –ù–µ–≥–∞—Ç–∏–≤", 0: "‚ûñ –ù–µ–π—Ç—Ä–∞–ª", 1: "üìà –ü–æ–∑–∏—Ç–∏–≤"}.get(sent, "‚ùì")
        print(f"  {label}: {count} ({count/len(valid_sentiments)*100:.1f}%)")

    print(f"\nüìä –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {valid_sentiments['confidence'].mean():.2f}/10")

    # –ü—Ä–∏–º–µ—Ä—ã
    print(f"\nüì∞ –ü–†–ò–ú–ï–†–´ –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
    for idx, row in valid_sentiments.head(3).iterrows():
        sent_label = {1: "üìà –ü–û–ó–ò–¢–ò–í", -1: "üìâ –ù–ï–ì–ê–¢–ò–í", 0: "‚ûñ –ù–ï–ô–¢–†–ê–õ"}.get(row['sentiment'], "‚ùì")
        print(f"\n{sent_label} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {row['confidence']}/10) | {row['tickers']}")
        print(f"  {row['title'][:80]}...")

# –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
start_str = START_DATE.replace('-', '') if START_DATE else 'all'
output_filename = f'news_2_with_tickers_llm_{start_str}.csv'
news_df.to_csv(output_filename, index=False)
print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_filename}")
print(f"{'='*60}")
